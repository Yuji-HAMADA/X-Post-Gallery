import asyncio
import json
import os
import sys
import argparse
import subprocess
from playwright.async_api import async_playwright

# extract_media.py ã‹ã‚‰å¿…è¦ãªé–¢æ•°ã¨å®šæ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# (åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™)
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from extract_media import extract_tweet_data, AUTH_PATH, DATA_DIR, OUTPUT_FILE

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-n", "--num", type=int, default=100, help="å–å¾—ã™ã‚‹æœ€å¤§ä»¶æ•°")
    parser.add_argument("--gist-id", type=str, default=None, help="æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®Gist ID (é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨)")
    return parser.parse_args()

def get_existing_ids_from_gist(gist_id):
    """Gistã‹ã‚‰æ—¢å­˜ã®data.jsonã‚’å–å¾—ã—ã€IDã®ã‚»ãƒƒãƒˆã‚’è¿”ã™"""
    if not gist_id:
        return set()
    
    print(f"ğŸ” Fetching existing data from Gist: {gist_id} ...")
    
    # èª­ã¿è¾¼ã¿ã‚’è©¦ã¿ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åã®å€™è£œ (å„ªå…ˆé †)
    candidate_files = ["data.json", "gallary_data.json", "tweets.js"]
    
    for filename in candidate_files:
        try:
            # gh ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã£ã¦ Gist ã®å†…å®¹ã‚’å–å¾—
            result = subprocess.run(
                ["gh", "gist", "view", gist_id, "--filename", filename, "--raw"],
                capture_output=True, text=True, check=True
            )
            
            raw_data = result.stdout.strip()
            
            # tweets.js ã®å ´åˆã€JSã®ä»£å…¥æ–‡ã‚’é™¤å»ã—ã¦JSONéƒ¨åˆ†ã‚’å–ã‚Šå‡ºã™ç°¡æ˜“å‡¦ç†
            if filename == "tweets.js" and "=" in raw_data:
                parts = raw_data.split('=', 1)
                if len(parts) > 1:
                    raw_data = parts[1].strip()

            data = json.loads(raw_data)
            
            ids = set()
            # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®åˆ¤å®š
            items = []
            if isinstance(data, dict) and "tweets" in data:
                items = data["tweets"]
            elif isinstance(data, list):
                items = data
            
            for item in items:
                tid = item.get("id_str")
                if not tid and "tweet" in item:
                    tid = item["tweet"].get("id_str")
                
                if tid:
                    ids.add(tid)
            
            print(f"âœ… Loaded {len(ids)} existing IDs from Gist (found '{filename}').")
            return ids

        except (subprocess.CalledProcessError, json.JSONDecodeError):
            continue

    print("âš ï¸ No valid data found in Gist. Proceeding as fresh run.")
    return set()

async def run():
    args = parse_args()
    if not os.path.exists(AUTH_PATH):
        print(f"âŒ Error: {AUTH_PATH} not found."); return

    os.makedirs(DATA_DIR, exist_ok=True)
    
    # 1. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®IDã‚’å–å¾—ï¼ˆåœæ­¢æ¡ä»¶ç”¨ï¼‰
    seen_ids_in_gist = get_existing_ids_from_gist(args.gist_id)

    url = "https://x.com/home"
    print(f"ğŸš€ Fetching 'For you' tweets from: {url}")

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(storage_state=AUTH_PATH)
        page = await context.new_page()
        
        await page.goto(url, wait_until="domcontentloaded")
        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã®åˆæœŸãƒ­ãƒ¼ãƒ‰å¾…æ©Ÿ
        await page.wait_for_timeout(5000)

        new_tweets = []
        current_run_ids = set() # ä»Šå›ã®å®Ÿè¡Œå†…ã§é‡è¤‡ã‚’é˜²ãç”¨
        stop_scraping = False
        no_new_data_count = 0
        
        while len(new_tweets) < args.num and not stop_scraping:
            articles = await page.query_selector_all('article')
            
            added_in_this_scroll = 0
            for article in articles:
                data = await extract_tweet_data(article)
                if not data: continue
                
                tid = data["tweet"]["id_str"]
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯: Gistã«ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—ã—ã¦æ¬¡ã¸
                if tid in seen_ids_in_gist:
                    continue

                # é‡è¤‡ãƒã‚§ãƒƒã‚¯: ä»Šå›ã™ã§ã«å–å¾—æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
                if tid in current_run_ids: continue
                
                current_run_ids.add(tid)
                new_tweets.append(data)
                added_in_this_scroll += 1
                print(f"  [{len(new_tweets)}] Saved: @{tid}")

                if len(new_tweets) >= args.num: 
                    stop_scraping = True
                    break
            
            if stop_scraping: break
            
            # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
            await page.mouse.wheel(0, 2000)
            await asyncio.sleep(3)
            
            # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢
            if added_in_this_scroll == 0:
                no_new_data_count += 1
                if no_new_data_count > 5:
                    print("âš ï¸ No new tweets found after scrolling multiple times. Stopping.")
                    break
            else:
                no_new_data_count = 0

        # ä¿å­˜ (extract_media.py ã¨åŒã˜å½¢å¼)
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            f.write("window.YTD.tweets.part0 = ")
            json.dump(new_tweets if new_tweets else [], f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… Done: {len(new_tweets)} new tweets saved locally.")
        await browser.close()

if __name__ == "__main__":
    asyncio.run(run())
