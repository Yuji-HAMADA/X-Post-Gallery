import asyncio
import json
import os
import sys
import argparse
from playwright.async_api import async_playwright

# extract_media.py ã‹ã‚‰å¿…è¦ãªé–¢æ•°ã¨å®šæ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# (åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™)
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from extract_media import extract_tweet_data, AUTH_PATH, DATA_DIR, OUTPUT_FILE

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-n", "--num", type=int, default=100, help="å–å¾—ã™ã‚‹æœ€å¤§ä»¶æ•°")
    parser.add_argument("--skip-ids-file", type=str, default="", help="ã‚¹ã‚­ãƒƒãƒ—ã™ã¹ãIDã®ãƒªã‚¹ãƒˆãŒæ›¸ã‹ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹")
    # gist-id ã®å¼•æ•°ã¯ä¸è¦ã«ãªã£ãŸãŸã‚å‰Šé™¤ï¼ˆã¾ãŸã¯äº’æ›æ€§ã®ãŸã‚æ®‹ã—ã¦ã‚‚è‰¯ã„ã§ã™ãŒã€ã“ã“ã§ã¯æ–°ã—ã„è¨­è¨ˆã«åˆã‚ã›ã¾ã™ï¼‰
    parser.add_argument("--gist-id", type=str, default=None, help="Deprecated: use --skip-ids-file instead")
    return parser.parse_args()

def load_skip_ids(skip_ids_file):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¹ã‚­ãƒƒãƒ—ã™ã¹ãæ—¢å­˜IDã®ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    skip_ids = set()
    if skip_ids_file and os.path.exists(skip_ids_file):
        with open(skip_ids_file, 'r', encoding='utf-8') as f:
            for line in f:
                tid = line.strip()
                if tid:
                    skip_ids.add(tid)
        print(f"âœ… Loaded {len(skip_ids)} skip IDs from {skip_ids_file}")
    return skip_ids

async def run():
    args = parse_args()
    if not os.path.exists(AUTH_PATH):
        print(f"âŒ Error: {AUTH_PATH} not found."); return

    os.makedirs(DATA_DIR, exist_ok=True)
    
    # 1. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®IDã‚’å–å¾—ï¼ˆåœæ­¢æ¡ä»¶ç”¨ï¼‰
    seen_ids_in_gist = load_skip_ids(args.skip_ids_file)

    url = "https://x.com/home"
    print(f"ğŸš€ Fetching 'For you' tweets from: {url}")

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(storage_state=AUTH_PATH)
        page = await context.new_page()
        
        await page.goto(url, wait_until="domcontentloaded")
        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã®åˆæœŸãƒ­ãƒ¼ãƒ‰å¾…æ©Ÿ
        await page.wait_for_timeout(5000)

        new_tweets = []
        current_run_ids = set() # ä»Šå›ã®å®Ÿè¡Œå†…ã§é‡è¤‡ã‚’é˜²ãç”¨
        stop_scraping = False
        no_new_data_count = 0
        
        while len(new_tweets) < args.num and not stop_scraping:
            articles = await page.query_selector_all('article')
            
            added_in_this_scroll = 0
            for article in articles:
                data = await extract_tweet_data(article)
                if not data: continue
                
                tid = data["tweet"]["id_str"]
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯: æ—¢å­˜ãƒªã‚¹ãƒˆï¼ˆMasterã®ä»£è¡¨ãƒã‚¹ãƒˆãªã©ï¼‰ã«ã‚ã‚‹ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
                if tid in seen_ids_in_gist:
                    continue

                # é‡è¤‡ãƒã‚§ãƒƒã‚¯: ä»Šå›ã™ã§ã«å–å¾—æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
                if tid in current_run_ids: continue
                
                current_run_ids.add(tid)
                new_tweets.append(data)
                added_in_this_scroll += 1
                print(f"  [{len(new_tweets)}] Saved: @{tid}")

                if len(new_tweets) >= args.num: 
                    stop_scraping = True
                    break
            
            if stop_scraping: break
            
            # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
            await page.mouse.wheel(0, 2000)
            await asyncio.sleep(3)
            
            # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢
            if added_in_this_scroll == 0:
                no_new_data_count += 1
                if no_new_data_count > 5:
                    print("âš ï¸ No new tweets found after scrolling multiple times. Stopping.")
                    break
            else:
                no_new_data_count = 0

        # ä¿å­˜ (extract_media.py ã¨åŒã˜å½¢å¼)
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            f.write("window.YTD.tweets.part0 = ")
            json.dump(new_tweets if new_tweets else [], f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… Done: {len(new_tweets)} new tweets saved locally.")
        await browser.close()

if __name__ == "__main__":
    asyncio.run(run())
