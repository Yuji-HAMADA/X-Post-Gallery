import asyncio
import json
import os
import re
import sys  # å¼•æ•°å–å¾—ç”¨
import argparse  # è¿½åŠ 
from playwright.async_api import async_playwright
from urllib.parse import quote

async def run():
    # --- å¼•æ•°ã®è§£æ ---
    parser = argparse.ArgumentParser()
    # -n ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ100ï¼‰
    parser.add_argument("-n", "--num", type=int, default=100, help="å–å¾—ä»¶æ•°")
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’å¼•æ•°ã§å—ã‘å–ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹
    parser.add_argument("-u", "--user", type=str, default="travelbeauty8", help="å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼ID")
    # å–å¾—ãƒ¢ãƒ¼ãƒ‰ï¼ˆæœªä½¿ç”¨ã ãŒå°†æ¥ã®æ‹¡å¼µç”¨ã«è¿½åŠ ï¼‰
    parser.add_argument("--mode", type=str, default="all", choices=["all", "post_only", "repost_only"], help="å–å¾—ãƒ¢ãƒ¼ãƒ‰")
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆIDï¼ˆä½ç½®å¼•æ•°ã¨ã—ã¦ç¶­æŒï¼‰
    parser.add_argument("target_id", nargs="?", default=None, help="åˆ°é”ã‚¿ãƒ¼ã‚²ãƒƒãƒˆID")
    
    args = parser.parse_args()
    MAX_LIMIT = args.num
    target_id = args.target_id
    target_user = args.user # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼

    # --- è¨­å®š ---
    DATA_DIR = "data"
    if not os.path.exists(DATA_DIR):
        os.makedirs(DATA_DIR)

    OUTPUT_FILE = os.path.join(DATA_DIR, "tweets.js")
    AUTH_PATH = os.path.join(DATA_DIR, "auth.json")
    
    if not os.path.exists(AUTH_PATH):
        print(f"âŒ Error: {AUTH_PATH} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        sys.exit(1)

    # ãƒ­ã‚°å‡ºåŠ›
    if target_id:
        print(f"ID: {target_id} ã«åˆ°é”ã™ã‚‹ã¾ã§å–å¾—ã—ã¾ã™ï¼ˆæœ€å¤§{MAX_LIMIT}ä»¶ï¼‰", flush=True)
    else:
        print(f"æœ€æ–°ã‹ã‚‰æœ€å¤§{MAX_LIMIT}ä»¶ã‚’å–å¾—ã—ã¾ã™", flush=True)

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)

        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–ï¼šãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒä¸å®‰å®šãªå ´åˆã«å‚™ãˆã¦å°‘ã—é•·ã‚ã«
        context = await browser.new_context(
            storage_state=AUTH_PATH,
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
        )
        page = await context.new_page()

        base_query = f"from:{target_user} filter:images"

        # --- ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®ã‚¯ã‚¨ãƒªæ±ºå®šï¼ˆã™ã¹ã¦æœ€æ–°é † f=live ãƒ™ãƒ¼ã‚¹ï¼‰ ---
        if args.mode == "post_only":
            # è‡ªåˆ†ã®ãƒã‚¹ãƒˆã®ã¿ï¼ˆç”»åƒã‚ã‚Šã€ãƒªãƒã‚¹ãƒˆãªã—ï¼‰
            query = f"from:{target_user} filter:images -filter:reposts"
            print(f"ğŸ¬ ãƒ¢ãƒ¼ãƒ‰: ãƒã‚¹ãƒˆã®ã¿", flush=True)

        elif args.mode == "repost_only":
            # ãƒªãƒã‚¹ãƒˆã®ã¿ï¼ˆå¾Œã§Pythonå´ã§ã•ã‚‰ã«å³å¯†ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
            query = f"from:{target_user} include:nativeretweets -filter:replies"
            print(f"ğŸ”„ ãƒ¢ãƒ¼ãƒ‰: ãƒªãƒã‚¹ãƒˆã®ã¿", flush=True)

        else:
            # å…¨éƒ¨ï¼ˆãƒã‚¹ãƒˆã‚‚ãƒªãƒã‚¹ãƒˆã‚‚ç”»åƒä»˜ãã§æœ€æ–°é †ï¼‰
            query = f"from:{target_user} include:nativeretweets -filter:replies"
            print(f"âœ¨ ãƒ¢ãƒ¼ãƒ‰: å…¨éƒ¨", flush=True)

        # ğŸš€ URLã®çµ„ã¿ç«‹ã¦
        encoded_query = quote(query)
        url = f"https://x.com/search?q={encoded_query}&f=live"
        
        print(f"ğŸš€ ã‚¢ã‚¯ã‚»ã‚¹URL: {url}")

        try:
            await page.goto(url, wait_until="domcontentloaded", timeout=60000)
        except Exception as e:
            print(f"èª­ã¿è¾¼ã¿ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ(ç¶šè¡Œ): {e}")

        await page.wait_for_timeout(10000)

        new_tweets = []
        seen_repost_ids = set()
        stop_scrolling = False

        while len(new_tweets) < MAX_LIMIT and not stop_scrolling:
            articles = await page.query_selector_all('article')
            
            for article in articles:
                links = await article.query_selector_all('a[href*="/status/"]')
                current_repost_id = ""
                origin_user = "unknown"
                origin_status_id = ""
                
                for link in links:
                    href = await link.get_attribute('href')
                    match = re.search(r'/([^/]+)/status/(\d+)', href)
                    if match:
                        if origin_user == "unknown":
                            origin_user = match.group(1)
                            origin_status_id = match.group(2)
                        current_repost_id = match.group(2)

                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                if not current_repost_id:
                    print(f"  [SKIP] ID:{current_repost_id} - ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹IDãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                    continue
                if current_repost_id in seen_repost_ids:
                    continue

                # IDä¸€è‡´ãƒã‚§ãƒƒã‚¯ï¼ˆå¼•æ•°ãŒã‚ã‚‹å ´åˆï¼‰
                if target_id and current_repost_id == target_id:
                    print(f"  [åˆ°é”] æŒ‡å®šIDä¸€è‡´: {current_repost_id}")
                    stop_scrolling = True
                    break

                # åˆ¤å®šç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                full_text_raw = await article.inner_text()
                
                # --- ãƒ‡ãƒãƒƒã‚°é–‹å§‹ ---
                is_repost = any(word in full_text_raw for word in ["ãƒªãƒã‚¹ãƒˆ", "Reposted", "reposted"])

                # --- ä¿®æ­£: repost_only ãƒ¢ãƒ¼ãƒ‰ã®æ™‚ã®æœ€çµ‚ã‚¬ãƒ¼ãƒ‰ ---
                if args.mode == "repost_only" and not is_repost:
                    # ãƒªãƒã‚¹ãƒˆã®ã¿æ¬²ã—ã„ã®ã«ã€ãƒªãƒã‚¹ãƒˆã˜ã‚ƒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    continue 

                # --- ä¿®æ­£: post_only ãƒ¢ãƒ¼ãƒ‰ã®æ™‚ï¼ˆå¿µã®ãŸã‚ï¼‰ ---
                if args.mode == "post_only" and is_repost:
                    continue

                seen_repost_ids.add(current_repost_id)
                
                # ç”»åƒè¦ç´ ã‚’å–å¾—
                all_images = await article.query_selector_all('[data-testid="tweetPhoto"] img')
                
                if not all_images:
                    # å®¹ç–‘è€…1: ç”»åƒãŒã‚ã‚‹ã¯ãšãªã®ã«å–å¾—ã§ãã¦ã„ãªã„ï¼ˆèª­ã¿è¾¼ã¿å¾…ã¡ãªã©ï¼‰
                    print(f"  [SKIP] ID:{current_repost_id} - ç”»åƒè¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    continue

                media_urls = []
                for img in all_images:
                    # 1. DOMåˆ¤å®šï¼ˆå¼•ç”¨ãƒ„ã‚¤ãƒ¼ãƒˆå†…ã«ã‚ã‚‹ã‹ï¼‰
                    is_excluded_dom = await img.evaluate("""(node) => {
                        return !!node.closest('[data-testid="quotedTweet"]') || 
                               !!node.closest('[data-testid="placementTracking"]');
                    }""")

                    # 2. URL/ã‚µã‚¤ã‚ºåˆ¤å®š
                    src = await img.get_attribute('src')
                    if not src:
                        print(f"  [SKIP-IMG] ID:{current_repost_id} - ç”»åƒã®srcå±æ€§ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                        continue

                    is_thumbnail = any(sz in src for sz in ["name=120x120", "name=240x240"])

                    # é™¤å¤–ç†ç”±ã‚’ãƒ­ã‚°ã«å‡ºã™ï¼ˆç¢ºèªç”¨ï¼‰
                    if is_excluded_dom:
                        print(f"  [SKIP-IMG] ID:{current_repost_id} - å¼•ç”¨ãƒ–ãƒ­ãƒƒã‚¯å†…ã®ãŸã‚é™¤å¤–")
                        continue
                    if is_thumbnail:
                        # ã“ã“ã§ small ãŒå‡ºãªããªã‚‹ã¯ãš
                        print(f"  [SKIP-IMG] ID:{current_repost_id} - ã‚µãƒ ãƒã‚¤ãƒ«ã‚µã‚¤ã‚º({src.split('name=')[-1]})ã®ãŸã‚é™¤å¤–")
                        continue

                    if "pbs.twimg.com/media/" in src:
                        if not any(m["media_url_https"] == src for m in media_urls):
                            media_urls.append({
                                "media_url_https": src, 
                                "type": "photo",
                                "expanded_url": f"https://x.com/{origin_user}/status/{origin_status_id}/photo/1"
                            })

                # ä¿å­˜å‡¦ç†
                if media_urls:
                    tweet_text_el = await article.query_selector('[data-testid="tweetText"]')
                    raw_text = await tweet_text_el.inner_text() if tweet_text_el else ""
                    
                    # --- ä¿®æ­£: ãƒªãƒã‚¹ãƒˆã‹ã©ã†ã‹ã§è¡¨ç¤ºãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ ---
                    if is_repost:
                        full_text = f"RT @{origin_user}: {raw_text}"
                    else:
                        full_text = f"@{origin_user}: {raw_text}"
                    
                    time_el = await article.query_selector('time')
                    timestamp = await time_el.get_attribute('datetime') if time_el else ""
                    
                    new_tweets.append({
                        "tweet": {
                            "id_str": current_repost_id,
                            "full_text": full_text,  # åˆ‡ã‚Šæ›¿ãˆãŸãƒ†ã‚­ã‚¹ãƒˆã‚’æ ¼ç´
                            "created_at": timestamp,
                            "entities": {
                                "user_mentions": [{ "screen_name": origin_user }],
                                "media": media_urls
                            },
                            "extended_entities": { "media": media_urls },
                            "source_status_url": f"https://x.com/{origin_user}/status/{origin_status_id}"
                        }
                    })
                    # ãƒ­ã‚°ã‚‚ãƒªãƒã‚¹ãƒˆã‹ã©ã†ã‹åˆ†ã‹ã‚Šã‚„ã™ãã™ã‚‹ã¨ä¾¿åˆ©ã§ã™
                    type_label = "RT" if is_repost else "Post"
                    print(f"  [{len(new_tweets)}] {type_label}å–å¾—ä¸­: @{origin_user}", flush=True)
                else:
                    print(f"  [SKIP] ID:{current_repost_id} - æœ‰åŠ¹ãªãƒ¡ã‚¤ãƒ³ç”»åƒãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

                if len(new_tweets) >= MAX_LIMIT:
                    break

            if len(new_tweets) < MAX_LIMIT and not stop_scrolling:
                await page.mouse.wheel(0, 2000)
                await asyncio.sleep(4)

        # ä¿å­˜
        if new_tweets:
            with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                f.write("window.YTD.tweets.part0 = ")
                json.dump(new_tweets, f, ensure_ascii=False, indent=2)
            print(f"\nå®Œäº†ï¼ {len(new_tweets)} ä»¶ä¿å­˜ã—ã¾ã—ãŸã€‚", flush=True)

        await browser.close()

if __name__ == "__main__":
    asyncio.run(run())