"""
æŒ‡å®šãƒ¦ãƒ¼ã‚¶(ã¾ãŸã¯ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°)ã®ãƒã‚¹ãƒˆã‚’å–å¾—ã—ã€æ—¢å­˜Gistã«ã‚¢ãƒšãƒ³ãƒ‰ã™ã‚‹ã€‚
  - æ—¢å­˜IDãŒè¦‹ã¤ã‹ã£ãŸã‚‰å–å¾—ã‚’åœæ­¢ï¼ˆ--stop-on-existingï¼‰
  - 3ç¨®é¡ã®Gistãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¯¾å¿œ:
      master : {user_screen_name, user_gists:{user:gist_id}, tweets:[flat]}
      multi  : {users:{user:{tweets:[]}}}
      single : {user_screen_name, tweets:[]}
  - masterå½¢å¼ã§å¯¾è±¡ãŒãƒ¦ãƒ¼ã‚¶ã®å ´åˆ:
      - æ—¢å­˜ãƒ¦ãƒ¼ã‚¶: user_gists ã«ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹Gistã¸è¿½è¨˜
      - æ–°è¦ãƒ¦ãƒ¼ã‚¶: ä»»æ„ã®æ—¢å­˜Gistã‚’é¸æŠã—ã€ãã“ã¸è¿½è¨˜
      - è¿½åŠ ã™ã‚‹ã“ã¨ã§ä¸Šé™(GIST_MAX_TWEETS)ã‚’è¶…ãˆã‚‹å ´åˆã¯ã€æ–°è¦Gistã‚’ä½œæˆã—ãã“ã¸ä¿å­˜ã™ã‚‹
      - ãƒã‚¹ã‚¿ãƒ¼Gistã«ã¯ä»£è¡¨1ä»¶ã¨gist_idå‚ç…§ã‚’ä¿æŒã™ã‚‹
"""
import json
import os
import re
import shutil
import sys
import argparse
import subprocess
import tempfile

DATA_DIR = "data"
TWEETS_JS = os.path.join(DATA_DIR, "tweets.js")
GIST_MAX_TWEETS = 1000  # ç§»å‹•å…ˆGistã®ä¸Šé™

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-g", "--gist-id", required=True, help="Appendå¯¾è±¡ã®Gist ID")
    parser.add_argument("-u", "--user", default=None, help="Target user ID (--user ã¾ãŸã¯ --hashtag ã®ã©ã¡ã‚‰ã‹å¿…é ˆ)")
    parser.add_argument("--hashtag", type=str, default=None, help="Target hashtag (#ãªã—)")
    parser.add_argument("-m", "--mode", default="post_only", choices=["all", "post_only"])
    parser.add_argument("-n", "--num", type=int, default=100, help="æœ€å¤§å–å¾—ä»¶æ•°")
    parser.add_argument("-s", "--stop-on-existing", action="store_true", help="æ—¢å­˜IDã«å½“ãŸã£ãŸã‚‰åœæ­¢ï¼ˆã‚¹ãƒˆãƒƒãƒ—ã‚ªãƒ³ãƒ¢ãƒ¼ãƒ‰ï¼‰")
    parser.add_argument("--force-empty", action="store_true", help="GistãŒ0ä»¶ã§ã‚‚å¼·åˆ¶ç¶šè¡Œï¼ˆé€šå¸¸ã¯å®‰å…¨ã®ãŸã‚ä¸­æ–­ï¼‰")
    parser.add_argument("-p", "--promote-gist-id", default=None,
                        help="ç§»å‹•å…ˆGist IDã‚’æ‰‹å‹•æŒ‡å®šï¼ˆçœç•¥æ™‚ã¯user_gistsã‹ã‚‰è‡ªå‹•é¸æŠï¼‰")
    return parser.parse_args()

# ---------------------------------------------------------------------------
# Gist ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆ¤å®š
# ---------------------------------------------------------------------------

def is_master_gist_format(data):
    """ãƒã‚¹ã‚¿ãƒ¼Gistå½¢å¼: {user_gists:{...}, tweets:[flat]} ã‹ã©ã†ã‹åˆ¤å®š"""
    return isinstance(data, dict) and "user_gists" in data

def is_multi_user_format(data):
    """ãƒãƒ«ãƒãƒ¦ãƒ¼ã‚¶å½¢å¼: {users:{username:{tweets:[]}}} ã‹ã©ã†ã‹åˆ¤å®š"""
    return isinstance(data, dict) and "users" in data and not is_master_gist_format(data)

def _tweet_belongs_to_user(tweet, user):
    """ãƒ„ã‚¤ãƒ¼ãƒˆãŒæŒ‡å®šãƒ¦ãƒ¼ã‚¶ã®ã‚‚ã®ã‹ã©ã†ã‹åˆ¤å®š"""
    if f"x.com/{user}/status/" in tweet.get("post_url", ""):
        return True
    if tweet.get("full_text", "").startswith(f"@{user}:"):
        return True
    return False

def get_user_tweets(data, user):
    """ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ã®ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè‡ªå‹•åˆ¤å®šï¼‰"""
    if is_master_gist_format(data):
        # flat tweetsã‹ã‚‰å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ã®ã‚‚ã®ã®ã¿æŠ½å‡º
        if not user:
            return data.get("tweets", [])
        return [t for t in data.get("tweets", []) if _tweet_belongs_to_user(t, user)]
    if is_multi_user_format(data):
        return data.get("users", {}).get(user, {}).get("tweets", [])
    # single-userå½¢å¼
    return data.get("tweets", []) if isinstance(data, dict) else (data if isinstance(data, list) else [])

# ---------------------------------------------------------------------------
# Gist å–å¾—
# ---------------------------------------------------------------------------

def _fetch_via_git_clone(gist_id, candidate_files):
    """raw_urlå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: git clone ã§Gistã‚’å–å¾—"""
    tmpdir = tempfile.mkdtemp(prefix="gist_clone_")
    try:
        result = subprocess.run(
            ["gh", "gist", "clone", gist_id, tmpdir],
            capture_output=True, text=True,
        )
        if result.returncode != 0:
            print(f"âŒ git cloneå¤±æ•—: {result.stderr}")
            sys.exit(1)
        for filename in candidate_files:
            filepath = os.path.join(tmpdir, filename)
            if os.path.exists(filepath):
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.loads(f.read(), strict=False)
                return filename, data
    except json.JSONDecodeError as e:
        print(f"âŒ JSON parse error after git clone: {e}")
        sys.exit(1)
    finally:
        shutil.rmtree(tmpdir, ignore_errors=True)
    print("âŒ No valid data found in Gist.")
    sys.exit(1)

def fetch_gist_data(gist_id):
    """Gistã‹ã‚‰JSONå–å¾—ã€‚(filename, full_data) ã‚’è¿”ã™ã€‚
    curlãŒå¤±æ•—ã—ãŸå ´åˆã¯ git clone ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚
    """
    try:
        result = subprocess.run(
            ["gh", "api", f"gists/{gist_id}"],
            capture_output=True, text=True, check=True,
        )
        gist_meta = json.loads(result.stdout)
    except (subprocess.CalledProcessError, json.JSONDecodeError) as e:
        print(f"âŒ Failed to fetch Gist metadata: {e}")
        sys.exit(1)

    candidate_files = ["data.json", "gallary_data.json"]
    files = gist_meta.get("files", {})

    for filename in candidate_files:
        if filename not in files:
            continue
        raw_url = files[filename].get("raw_url")
        if not raw_url:
            continue
        try:
            token = os.environ.get("GH_TOKEN", "")
            curl_cmd = ["curl", "-sf", "-L"]  # -f: HTTPã‚¨ãƒ©ãƒ¼æ™‚ã«å¤±æ•—æ‰±ã„
            if token:
                curl_cmd += ["-H", f"Authorization: Bearer {token}"]
            curl_cmd.append(raw_url)
            dl = subprocess.run(curl_cmd, capture_output=True, text=True)
            if dl.returncode == 0:
                data = json.loads(dl.stdout)
                return filename, data
        except json.JSONDecodeError:
            pass

    # Fallback: git clone
    print("âš ï¸  raw_urlå–å¾—å¤±æ•— â†’ git clone ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯...")
    return _fetch_via_git_clone(gist_id, candidate_files)

# ---------------------------------------------------------------------------
# ç§»å‹•å…ˆGisté¸æŠ
# ---------------------------------------------------------------------------

def select_promote_gist_from_master(full_data):
    """user_gists ã®æœ«å°¾ï¼ˆæœ€è¿‘è¿½åŠ ï¼‰ã®ä¸€æ„ãªGist IDã‚’è‡ªå‹•é¸æŠã—ã¦è¿”ã™"""
    user_gists = full_data.get("user_gists", {})
    if not user_gists:
        return None
    seen = set()
    unique_gists = []
    for gist_id in user_gists.values():
        if gist_id not in seen:
            unique_gists.append(gist_id)
            seen.add(gist_id)
    return unique_gists[-1] if unique_gists else None

# ---------------------------------------------------------------------------
# IDç®¡ç†
# ---------------------------------------------------------------------------

def get_existing_ids_ordered(tweets):
    """æ—¢å­˜tweetsã‹ã‚‰é †åºä»˜ãIDãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰ï¼ˆé€£ç¶šä¸€è‡´åˆ¤å®šç”¨ï¼‰"""
    ids = []
    seen = set()
    for item in tweets:
        tid = item.get("id_str") or item.get("tweet", {}).get("id_str")
        if tid and tid not in seen:
            ids.append(tid)
            seen.add(tid)
    return ids

def write_skip_ids_file(ordered_ids):
    """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ—¢å­˜IDã‚’é †åºä»˜ãã§æ›¸ãå‡ºã™ï¼ˆé€£ç¶šä¸€è‡´åˆ¤å®šç”¨ï¼‰"""
    fd, path = tempfile.mkstemp(suffix=".txt", prefix="skip_ids_")
    with os.fdopen(fd, 'w') as f:
        for tid in ordered_ids:
            f.write(tid + "\n")
    return path

# ---------------------------------------------------------------------------
# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
# ---------------------------------------------------------------------------

def run_extraction(user, hashtag, mode, num, skip_ids_file, stop_on_existing=True):
    """extract_media.py ã‚’å‘¼ã³å‡ºã—ã¦ãƒã‚¹ãƒˆã‚’å–å¾—"""
    cmd = [
        sys.executable, "scripts/extract_media.py",
        "--mode", mode,
        "-n", str(num),
        "--skip-ids-file", skip_ids_file,
    ]
    if user:
        cmd.extend(["-u", user])
    elif hashtag:
        cmd.extend(["--hashtag", hashtag])
    if stop_on_existing:
        cmd.append("--stop-on-existing")
    print(f"ğŸš€ Running: {' '.join(cmd)}")
    result = subprocess.run(cmd)
    if result.returncode != 0:
        print("âŒ Extraction failed.")
        sys.exit(1)

def parse_tweets_js():
    """extract_media.py ãŒå‡ºåŠ›ã—ãŸ tweets.js ã‚’èª­ã‚€"""
    if not os.path.exists(TWEETS_JS):
        print("âŒ tweets.js not found.")
        sys.exit(1)
    with open(TWEETS_JS, 'r', encoding='utf-8') as f:
        content = f.read()
    json_str = re.sub(r'^window\.YTD\.tweets\.part0\s*=\s*', '', content)
    raw_tweets = json.loads(json_str)

    converted = []
    for item in raw_tweets:
        tweet = item.get('tweet', {})
        full_text = tweet.get('full_text', '')
        media_list = tweet.get('extended_entities', {}).get('media', [])
        if not media_list:
            media_list = tweet.get('entities', {}).get('media', [])
        if not media_list:
            continue
        media_urls = [m.get('media_url_https', '') for m in media_list if m.get('media_url_https')]
        if not media_urls:
            continue

        post_url = ''
        for m in media_list:
            eu = m.get('expanded_url', '')
            if '/status/' in eu:
                post_url = re.sub(r'/photo/\d+$', '', eu)
                break

        entry = {
            'full_text': full_text,
            'created_at': tweet.get('created_at', ''),
            'media_urls': media_urls,
            'id_str': tweet.get('id_str', ''),
        }
        if post_url:
            entry['post_url'] = post_url
        converted.append(entry)

    return converted

# ---------------------------------------------------------------------------
# ãƒãƒ¼ã‚¸
# ---------------------------------------------------------------------------

def append_tweets(existing_tweets, new_tweets):
    """æ–°è¦tweetsã‚’ã™ã¹ã¦å…ˆé ­ã«æŒ¿å…¥"""
    if not new_tweets:
        print("â„¹ï¸ No new tweets to append.")
        return existing_tweets

    existing_ids = {t.get("id_str") for t in existing_tweets if t.get("id_str")}
    unique_new = [t for t in new_tweets if t.get("id_str") not in existing_ids]
    if not unique_new:
        print("â„¹ï¸ All tweets already exist. Nothing to append.")
        return existing_tweets

    result = unique_new + existing_tweets
    print(f"âœ¨ Appended: {len(unique_new)} new tweets to head")
    return result

# ---------------------------------------------------------------------------
# å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
# ---------------------------------------------------------------------------

def build_output(full_data, user, merged_tweets, user_gist_id=None):
    """æœ€çµ‚å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰ã€‚å„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä¿æŒã™ã‚‹ã€‚
    user_gist_id ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆï¼ˆç§»å‹•å¾Œï¼‰:
      - master: flatãƒªã‚¹ãƒˆã‹ã‚‰è©²å½“ãƒ¦ãƒ¼ã‚¶ã‚’é™¤ãã€ä»£è¡¨1ä»¶ï¼ˆgist_idãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä»˜ãï¼‰ã‚’å…ˆé ­ã«è¿½åŠ 
      - multi : users[user] = {gist_id, tweets:[ä»£è¡¨1ä»¶]}
    """
    if not merged_tweets:
        return full_data

    if is_master_gist_format(full_data) and user:
        updated = dict(full_data)
        # ä»–ãƒ¦ãƒ¼ã‚¶ã®ãƒ„ã‚¤ãƒ¼ãƒˆã¯ãã®ã¾ã¾ä¿æŒ
        other_tweets = [t for t in updated.get("tweets", []) if not _tweet_belongs_to_user(t, user)]
        if user_gist_id:
            # ä»£è¡¨1ä»¶ã«gist_idã‚’ä»˜ä¸ã—ã¦flatãƒªã‚¹ãƒˆã®å…ˆé ­ã¸
            representative = dict(merged_tweets[0])
            representative["gist_id"] = user_gist_id
            updated["tweets"] = [representative] + other_tweets
            # user_gists ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚‚è¿½åŠ 
            user_gists = dict(updated.get("user_gists", {}))
            user_gists[user] = user_gist_id
            updated["user_gists"] = user_gists
        else:
            updated["tweets"] = merged_tweets + other_tweets
        return updated

    if is_multi_user_format(full_data) and user:
        updated = dict(full_data)
        users = dict(updated.get("users", {}))
        if user_gist_id:
            users[user] = {
                "gist_id": user_gist_id,
                "tweets": [merged_tweets[0]],
            }
            if "user_gists" in updated:
                user_gists = dict(updated["user_gists"])
                user_gists[user] = user_gist_id
                updated["user_gists"] = user_gists
        else:
            users[user] = {"tweets": merged_tweets}
        updated["users"] = users
        return updated

    # single-userå½¢å¼
    user_screen_name = full_data.get("user_screen_name", user or "Unknown") if isinstance(full_data, dict) else (user or "Unknown")
    return {
        "user_screen_name": user or user_screen_name,
        "tweets": merged_tweets,
    }

# ---------------------------------------------------------------------------
# Gistä½œæˆãƒ»ç§»å‹•
# ---------------------------------------------------------------------------

def create_gist_for_user(user, tweets):
    """æ–°ã—ã„Gistã‚’ä½œæˆã—ã¦ãƒ¦ãƒ¼ã‚¶ã®ãƒ„ã‚¤ãƒ¼ãƒˆã‚’æ ¼ç´ã—ã€Gist IDã‚’è¿”ã™"""
    data = {"users": {user: {"tweets": tweets}}}
    fd, tmp_file = tempfile.mkstemp(suffix=".json", prefix=f"new_gist_{user}_")
    try:
        with os.fdopen(fd, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        result = subprocess.run(
            ["gh", "gist", "create", tmp_file, "-p",
             "--filename", "data.json",
             "-d", f"Gallery Data for @{user}"],
            capture_output=True, text=True,
        )
    finally:
        if os.path.exists(tmp_file):
            os.unlink(tmp_file)

    if result.returncode != 0:
        print(f"âŒ æ–°è¦Gistä½œæˆå¤±æ•—: {result.stderr}")
        sys.exit(1)

    gist_url = result.stdout.strip().rstrip("/")
    new_gist_id = gist_url.split("/")[-1]
    return new_gist_id

def update_or_migrate_user_gist(promote_gist_id, promote_filename, promote_data, user, merged_tweets):
    """ãƒ¦ãƒ¼ã‚¶Gistã‚’æ›´æ–°ã™ã‚‹ã€‚GIST_MAX_TWEETSã‚’è¶…ãˆã‚‹å ´åˆã¯æ–°è¦Gistã‚’ä½œæˆã—ç§»å‹•ã™ã‚‹ã€‚"""
    if is_multi_user_format(promote_data):
        current_total = sum(
            len(u.get("tweets", [])) for u_name, u in promote_data.get("users", {}).items() if u_name != user
        )
    elif isinstance(promote_data, dict):
        current_total = 0  # single-user format ã®å ´åˆã¯ä»–ãƒ¦ãƒ¼ã‚¶ã®ãƒ„ã‚¤ãƒ¼ãƒˆã¯0ä»¶
    else:
        current_total = 0

    print(f"ğŸ“Š å¯¾è±¡ãƒ¦ãƒ¼ã‚¶Gist ({promote_gist_id[:8]}...): ä»–ãƒ¦ãƒ¼ã‚¶ã®ãƒ„ã‚¤ãƒ¼ãƒˆ {current_total} ä»¶, ä»Šå›ä¿å­˜ {len(merged_tweets)} ä»¶")

    if current_total + len(merged_tweets) > GIST_MAX_TWEETS:
        print(f"âš ï¸  è¿½åŠ ã™ã‚‹ã¨ {GIST_MAX_TWEETS} ä»¶ã‚’è¶…ãˆã‚‹ãŸã‚ã€æ–°è¦Gistã‚’ä½œæˆã—ã¾ã™...")
        new_gist_id = create_gist_for_user(user, merged_tweets)
        print(f"ğŸ†• æ–°è¦Gistä½œæˆ: {new_gist_id}  (@{user}: {len(merged_tweets)} ä»¶)")
        
        # å¤ã„Gistã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã™ã‚‹ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼‰
        if is_multi_user_format(promote_data) and user in promote_data.get("users", {}):
            updated_promote = dict(promote_data)
            del updated_promote["users"][user]
            fd, tmp_file = tempfile.mkstemp(suffix=".json", prefix="promote_del_")
            try:
                with os.fdopen(fd, 'w', encoding='utf-8') as f:
                    json.dump(updated_promote, f, ensure_ascii=False, indent=2)
                subprocess.run(["gh", "gist", "edit", promote_gist_id, "-f", promote_filename, tmp_file])
                print(f"ğŸ§¹ å¤ã„Gist ({promote_gist_id}) ã‹ã‚‰ @{user} ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
            finally:
                if os.path.exists(tmp_file):
                    os.unlink(tmp_file)
        
        return new_gist_id

    # æ—¢å­˜ã® promote Gist ã«è¿½åŠ 
    if is_multi_user_format(promote_data):
        updated_promote = dict(promote_data)
        users = dict(updated_promote.get("users", {}))
        users[user] = {"tweets": merged_tweets}
        updated_promote["users"] = users
    else:
        updated_promote = {"users": {user: {"tweets": merged_tweets}}}

    fd, tmp_file = tempfile.mkstemp(suffix=".json", prefix="promote_upd_")
    try:
        with os.fdopen(fd, 'w', encoding='utf-8') as f:
            json.dump(updated_promote, f, ensure_ascii=False, indent=2)
        result = subprocess.run(
            ["gh", "gist", "edit", promote_gist_id, "-f", promote_filename, tmp_file],
            capture_output=True, text=True,
        )
    finally:
        if os.path.exists(tmp_file):
            os.unlink(tmp_file)

    if result.returncode != 0:
        print(f"âŒ å¯¾è±¡ãƒ¦ãƒ¼ã‚¶Gistæ›´æ–°å¤±æ•—: {result.stderr}")
        sys.exit(1)

    after_total = current_total + len(merged_tweets)
    print(f"âœ… @{user} ã®ãƒ‡ãƒ¼ã‚¿ã‚’ Gist {promote_gist_id} ã«ä¿å­˜ã—ã¾ã—ãŸ (åˆè¨ˆ {after_total} ä»¶)")
    return promote_gist_id

# ---------------------------------------------------------------------------
# ãƒ¡ã‚¤ãƒ³
# ---------------------------------------------------------------------------

def main():
    args = parse_args()

    if not args.user and not args.hashtag:
        print("âŒ Error: --user ã¾ãŸã¯ --hashtag ã®ã©ã¡ã‚‰ã‹ãŒå¿…è¦ã§ã™ã€‚")
        sys.exit(1)

    target_label = f"#{args.hashtag}" if args.hashtag else f"@{args.user}"
    print(f"ğŸ¯ Target: {target_label}")

    # 1. æ—¢å­˜Gistãƒ‡ãƒ¼ã‚¿å–å¾—
    gist_filename, full_data = fetch_gist_data(args.gist_id)

    master = is_master_gist_format(full_data)
    
    if master and args.user:
        # ãƒã‚¹ã‚¿ãƒ¼Gist & ãƒ¦ãƒ¼ã‚¶æŒ‡å®š ã®å ´åˆ: å¸¸ã«ãƒ¦ãƒ¼ã‚¶Gistã¸ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦è¿½åŠ å‡¦ç†ã‚’è¡Œã†
        user_gists = full_data.get("user_gists", {})
        is_existing_user = args.user in user_gists
        
        promote_gist_id = args.promote_gist_id
        if not promote_gist_id:
            if is_existing_user:
                promote_gist_id = user_gists[args.user]
            else:
                promote_gist_id = select_promote_gist_from_master(full_data)
                
        if not promote_gist_id:
            print("âš ï¸ æ—¢å­˜ã®ãƒ¦ãƒ¼ã‚¶GistãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
            promote_gist_id = create_gist_for_user(args.user, [])
            
        print(f"ğŸ“Œ å¯¾è±¡ãƒ¦ãƒ¼ã‚¶Gist: {promote_gist_id} (æ—¢å­˜ãƒ¦ãƒ¼ã‚¶: {is_existing_user})")
        
        # ãƒ¦ãƒ¼ã‚¶Gistã‹ã‚‰æ—¢å­˜ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å–å¾—
        promote_filename, promote_data = fetch_gist_data(promote_gist_id)
        existing_tweets = get_user_tweets(promote_data, args.user)
        
        # å®‰å…¨ãƒã‚§ãƒƒã‚¯
        if len(existing_tweets) == 0 and not args.force_empty and not is_existing_user:
             # æ–°è¦ãƒ¦ãƒ¼ã‚¶ãªã‚‰0ä»¶ã§ã‚‚OK
             pass
        elif len(existing_tweets) == 0 and not args.force_empty:
             print("âš ï¸ è­¦å‘Š: æ—¢å­˜ãƒ¦ãƒ¼ã‚¶ã®ã¯ãšã§ã™ãŒGistå†…ã®Tweetæ•°ãŒ0ä»¶ã§ã™ã€‚")
             print("   æ„å›³çš„ã«ç©ºã®Gistã¸Appendã—ãŸã„å ´åˆã¯ --force-empty ã‚’ä»˜ã‘ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
             sys.exit(1)

        existing_ids_ordered = get_existing_ids_ordered(existing_tweets)
        skip_ids_file = write_skip_ids_file(existing_ids_ordered)
        
        try:
            # æ–°è¦ãƒã‚¹ãƒˆå–å¾—
            run_extraction(args.user, args.hashtag, args.mode, args.num, skip_ids_file, args.stop_on_existing)
        finally:
            os.unlink(skip_ids_file)

        new_tweets = parse_tweets_js()
        print(f"ğŸ“¥ New tweets extracted: {len(new_tweets)}")

        merged = append_tweets(existing_tweets, new_tweets)
        
        # ãƒ¦ãƒ¼ã‚¶Gistã‚’æ›´æ–°ãƒ»ç§»å‹•
        final_user_gist_id = update_or_migrate_user_gist(promote_gist_id, promote_filename, promote_data, args.user, merged)
        
        # ãƒã‚¹ã‚¿ãƒ¼Gistç”¨ã«ä»£è¡¨1ä»¶ã‚’ä¿æŒã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
        final_output = build_output(full_data, args.user, merged, user_gist_id=final_user_gist_id)
        
        output_file = "assets/data/data.json"
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_output, f, ensure_ascii=False, indent=2)

        print(f"â˜ï¸ Updating Master Gist ({args.gist_id})...")
        result = subprocess.run(
            ["gh", "gist", "edit", args.gist_id, "-f", gist_filename, output_file],
            capture_output=True, text=True,
        )
        if result.returncode == 0:
            print(f"âœ… Master Gist updated! @{args.user} â†’ {final_user_gist_id}")
        else:
            print(f"âŒ Master Gist update failed: {result.stderr}")
            sys.exit(1)

    else:
        # ãƒã‚¹ã‚¿ãƒ¼Gistã§ã¯ãªã„ã€ã¾ãŸã¯ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã®å ´åˆï¼ˆå¾“æ¥ã®ã‚·ãƒ³ãƒ—ãƒ«ãªè¿½åŠ å‡¦ç†ï¼‰
        existing_tweets = get_user_tweets(full_data, args.user)
        existing_ids_ordered = get_existing_ids_ordered(existing_tweets)
        
        if len(existing_tweets) == 0 and not args.force_empty:
            print("âš ï¸ è­¦å‘Š: Gistå†…ã®Tweetæ•°ãŒ0ä»¶ã§ã™ã€‚")
            print("   æ„å›³çš„ã«ç©ºã®Gistã¸Appendã—ãŸã„å ´åˆã¯ --force-empty ã‚’ä»˜ã‘ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            sys.exit(1)

        skip_ids_file = write_skip_ids_file(existing_ids_ordered)
        try:
            run_extraction(args.user, args.hashtag, args.mode, args.num, skip_ids_file, args.stop_on_existing)
        finally:
            os.unlink(skip_ids_file)

        new_tweets = parse_tweets_js()
        print(f"ğŸ“¥ New tweets extracted: {len(new_tweets)}")

        merged = append_tweets(existing_tweets, new_tweets)
        final_output = build_output(full_data, args.user, merged, user_gist_id=None)

        output_file = "assets/data/data.json"
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_output, f, ensure_ascii=False, indent=2)

        print(f"â˜ï¸ Updating Gist ({args.gist_id})...")
        result = subprocess.run(
            ["gh", "gist", "edit", args.gist_id, "-f", gist_filename, output_file],
            capture_output=True, text=True,
        )
        if result.returncode == 0:
            print(f"âœ… Gist updated successfully! Total for {target_label}: {len(merged)} tweets")
        else:
            print(f"âŒ Gist update failed: {result.stderr}")
            sys.exit(1)

if __name__ == "__main__":
    main()
