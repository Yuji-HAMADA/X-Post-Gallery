"""
æŒ‡å®šãƒ¦ãƒ¼ã‚¶(ã¾ãŸã¯ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°)ã®ãƒã‚¹ãƒˆã‚’å–å¾—ã—ã€æ—¢å­˜Gistã«ã‚¢ãƒšãƒ³ãƒ‰ã™ã‚‹ã€‚
  - æ—¢å­˜IDãŒè¦‹ã¤ã‹ã£ãŸã‚‰å–å¾—ã‚’åœæ­¢ï¼ˆ--stop-on-existingï¼‰
  - Gistãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯ master ã®ã¿å¯¾å¿œ:
      {user_screen_name, user_gists:{user:gist_id}, tweets:[flat]}
  - masterå½¢å¼ã§å¯¾è±¡ãŒãƒ¦ãƒ¼ã‚¶ã®å ´åˆ:
      - æ—¢å­˜ãƒ¦ãƒ¼ã‚¶: user_gists ã«ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹Gistã¸è¿½è¨˜
      - æ–°è¦ãƒ¦ãƒ¼ã‚¶: ä»»æ„ã®æ—¢å­˜Gistã‚’é¸æŠã—ã€ãã“ã¸è¿½è¨˜
      - è¿½åŠ ã™ã‚‹ã“ã¨ã§ä¸Šé™(GIST_MAX_TWEETS)ã‚’è¶…ãˆã‚‹å ´åˆã¯ã€æ–°è¦Gistã‚’ä½œæˆã—ãã“ã¸ä¿å­˜ã™ã‚‹
      - ãƒã‚¹ã‚¿ãƒ¼Gistã«ã¯ä»£è¡¨1ä»¶ã¨gist_idå‚ç…§ã‚’ä¿æŒã™ã‚‹
  - ForYouã‚¿ãƒ–å–å¾—ï¼ˆ--foryouï¼‰ã‚„ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ãªã©ã§è¤‡æ•°ãƒ¦ãƒ¼ã‚¶ãŒæ··åœ¨ã™ã‚‹å ´åˆ:
      - å–å¾—ã—ãŸãƒã‚¹ãƒˆã‚’ãƒ¦ãƒ¼ã‚¶ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
      - å„ãƒ¦ãƒ¼ã‚¶ã«ã¤ã„ã¦ã€å¯¾å¿œã™ã‚‹Gistã¸è¿½è¨˜ãƒ»ç§»å‹•ã‚’è¡Œã†
      - ãƒã‚¹ã‚¿ãƒ¼Gistã® user_gists ã¨ä»£è¡¨ãƒã‚¹ãƒˆã‚’æ›´æ–°
"""
import json
import os
import re
import shutil
import sys
import argparse
import subprocess
import tempfile

DATA_DIR = "data"
TWEETS_JS = os.path.join(DATA_DIR, "tweets.js")
GIST_MAX_TWEETS = 1000  # ç§»å‹•å…ˆGistã®ä¸Šé™
USER_PATTERN = re.compile(r"^@([^:]+):")

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-g", "--gist-id", required=True, help="Appendå¯¾è±¡ã®Gist ID")
    parser.add_argument("-u", "--user", default=None, help="Target user ID")
    parser.add_argument("--hashtag", type=str, default=None, help="Target hashtag (#ãªã—)")
    parser.add_argument("--foryou", action="store_true", help="For Youã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‹ã‚‰å–å¾—")
    parser.add_argument("-m", "--mode", default="post_only", choices=["all", "post_only"])
    parser.add_argument("-n", "--num", type=int, default=100, help="æœ€å¤§å–å¾—ä»¶æ•°")
    parser.add_argument("-s", "--stop-on-existing", action="store_true", help="æ—¢å­˜IDã«å½“ãŸã£ãŸã‚‰åœæ­¢ï¼ˆã‚¹ãƒˆãƒƒãƒ—ã‚ªãƒ³ãƒ¢ãƒ¼ãƒ‰ï¼‰")
    parser.add_argument("--force-empty", action="store_true", help="GistãŒ0ä»¶ã§ã‚‚å¼·åˆ¶ç¶šè¡Œ")
    parser.add_argument("-p", "--promote-gist-id", default=None,
                        help="ç§»å‹•å…ˆGist IDã‚’æ‰‹å‹•æŒ‡å®šï¼ˆçœç•¥æ™‚ã¯user_gistsã‹ã‚‰è‡ªå‹•é¸æŠï¼‰")
    return parser.parse_args()

def extract_username(tweet):
    """ãƒ„ã‚¤ãƒ¼ãƒˆã‹ã‚‰ãƒ¦ãƒ¼ã‚¶åã‚’æŠ½å‡º"""
    m = USER_PATTERN.match(tweet.get("full_text", ""))
    if m:
        return m.group(1).strip()
    post_url = tweet.get("post_url", "")
    m2 = re.search(r"x\.com/([^/]+)/status/", post_url)
    if m2:
        return m2.group(1)
    return "Unknown"

def group_tweets_by_user(tweets):
    """ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ãƒ¦ãƒ¼ã‚¶åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–"""
    groups = {}
    for tweet in tweets:
        user = extract_username(tweet)
        groups.setdefault(user, []).append(tweet)
    return groups

# ---------------------------------------------------------------------------
# Gist ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆ¤å®š
# ---------------------------------------------------------------------------

def is_master_gist_format(data):
    """ãƒã‚¹ã‚¿ãƒ¼Gistå½¢å¼: {user_gists:{...}, tweets:[flat]} ã‹ã©ã†ã‹åˆ¤å®š"""
    return isinstance(data, dict) and "user_gists" in data

def _tweet_belongs_to_user(tweet, user):
    """ãƒ„ã‚¤ãƒ¼ãƒˆãŒæŒ‡å®šãƒ¦ãƒ¼ã‚¶ã®ã‚‚ã®ã‹ã©ã†ã‹åˆ¤å®š"""
    if f"x.com/{user}/status/" in tweet.get("post_url", ""):
        return True
    if tweet.get("full_text", "").startswith(f"@{user}:"):
        return True
    return False

def get_user_tweets(data, user):
    """ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ã®ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å–å¾—ï¼ˆæ—§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰ã®ç§»è¡Œå¯¾å¿œï¼‰"""
    if is_master_gist_format(data):
        if not user:
            return data.get("tweets", [])
        return [t for t in data.get("tweets", []) if _tweet_belongs_to_user(t, user)]
    
    if isinstance(data, dict):
        if "users" in data:
            return data.get("users", {}).get(user, {}).get("tweets", [])
        return data.get("tweets", [])
    if isinstance(data, list):
        return data
    return []

# ---------------------------------------------------------------------------
# Gist å–å¾—
# ---------------------------------------------------------------------------

def _fetch_via_git_clone(gist_id, candidate_files):
    tmpdir = tempfile.mkdtemp(prefix="gist_clone_")
    try:
        result = subprocess.run(
            ["gh", "gist", "clone", gist_id, tmpdir],
            capture_output=True, text=True,
        )
        if result.returncode != 0:
            print(f"âŒ git cloneå¤±æ•—: {result.stderr}")
            sys.exit(1)
        for filename in candidate_files:
            filepath = os.path.join(tmpdir, filename)
            if os.path.exists(filepath):
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.loads(f.read(), strict=False)
                return filename, data
    except json.JSONDecodeError as e:
        print(f"âŒ JSON parse error after git clone: {e}")
        sys.exit(1)
    finally:
        shutil.rmtree(tmpdir, ignore_errors=True)
    print("âŒ No valid data found in Gist.")
    sys.exit(1)

def fetch_gist_data(gist_id):
    try:
        result = subprocess.run(
            ["gh", "api", f"gists/{gist_id}"],
            capture_output=True, text=True, check=True,
        )
        gist_meta = json.loads(result.stdout)
    except (subprocess.CalledProcessError, json.JSONDecodeError) as e:
        print(f"âŒ Failed to fetch Gist metadata: {e}")
        sys.exit(1)

    candidate_files = ["data.json", "gallary_data.json"]
    files = gist_meta.get("files", {})

    for filename in candidate_files:
        if filename not in files:
            continue
        raw_url = files[filename].get("raw_url")
        if not raw_url:
            continue
        try:
            token = os.environ.get("GH_TOKEN", "")
            curl_cmd = ["curl", "-sf", "-L"]
            if token:
                curl_cmd += ["-H", f"Authorization: Bearer {token}"]
            curl_cmd.append(raw_url)
            dl = subprocess.run(curl_cmd, capture_output=True, text=True)
            if dl.returncode == 0:
                data = json.loads(dl.stdout)
                return filename, data
        except json.JSONDecodeError:
            pass

    print("âš ï¸  raw_urlå–å¾—å¤±æ•— â†’ git clone ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯...")
    return _fetch_via_git_clone(gist_id, candidate_files)

# ---------------------------------------------------------------------------
# ç§»å‹•å…ˆGisté¸æŠ
# ---------------------------------------------------------------------------

def select_promote_gist_from_master(full_data):
    user_gists = full_data.get("user_gists", {})
    if not user_gists:
        return None
    seen = set()
    unique_gists = []
    for gist_id in user_gists.values():
        if gist_id not in seen:
            unique_gists.append(gist_id)
            seen.add(gist_id)
    return unique_gists[-1] if unique_gists else None

# ---------------------------------------------------------------------------
# IDç®¡ç†
# ---------------------------------------------------------------------------

def get_existing_ids_ordered(tweets):
    ids = []
    seen = set()
    for item in tweets:
        tid = item.get("id_str") or item.get("tweet", {}).get("id_str")
        if tid and tid not in seen:
            ids.append(tid)
            seen.add(tid)
    return ids

def write_skip_ids_file(ordered_ids):
    fd, path = tempfile.mkstemp(suffix=".txt", prefix="skip_ids_")
    with os.fdopen(fd, 'w') as f:
        for tid in ordered_ids:
            f.write(tid + "\n")
    return path

# ---------------------------------------------------------------------------
# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
# ---------------------------------------------------------------------------

def run_extraction(args, skip_ids_file):
    if args.foryou:
        cmd = [
            sys.executable, "scripts/extract_foryou.py",
            "-n", str(args.num),
            "--skip-ids-file", skip_ids_file,
        ]
        print(f"ğŸš€ Running ForYou Extraction: {' '.join(cmd)}")
    else:
        cmd = [
            sys.executable, "scripts/extract_media.py",
            "--mode", args.mode,
            "-n", str(args.num),
            "--skip-ids-file", skip_ids_file,
        ]
        if args.user:
            cmd.extend(["-u", args.user])
        elif args.hashtag:
            cmd.extend(["--hashtag", args.hashtag])
        if args.stop_on_existing:
            cmd.append("--stop-on-existing")
        print(f"ğŸš€ Running Media Extraction: {' '.join(cmd)}")
        
    result = subprocess.run(cmd)
    if result.returncode != 0:
        print("âŒ Extraction failed.")
        sys.exit(1)

def parse_tweets_js():
    if not os.path.exists(TWEETS_JS):
        print("âŒ tweets.js not found.")
        sys.exit(1)
    with open(TWEETS_JS, 'r', encoding='utf-8') as f:
        content = f.read()
    json_str = re.sub(r'^window\.YTD\.tweets\.part0\s*=\s*', '', content)
    raw_tweets = json.loads(json_str)

    converted = []
    for item in raw_tweets:
        tweet = item.get('tweet', {})
        full_text = tweet.get('full_text', '')
        media_list = tweet.get('extended_entities', {}).get('media', [])
        if not media_list:
            media_list = tweet.get('entities', {}).get('media', [])
        if not media_list:
            continue
        media_urls = [m.get('media_url_https', '') for m in media_list if m.get('media_url_https')]
        if not media_urls:
            continue

        post_url = ''
        for m in media_list:
            eu = m.get('expanded_url', '')
            if '/status/' in eu:
                post_url = re.sub(r'/photo/\d+$', '', eu)
                break

        entry = {
            'full_text': full_text,
            'created_at': tweet.get('created_at', ''),
            'media_urls': media_urls,
            'id_str': tweet.get('id_str', ''),
        }
        if post_url:
            entry['post_url'] = post_url
        converted.append(entry)

    return converted

# ---------------------------------------------------------------------------
# ãƒãƒ¼ã‚¸
# ---------------------------------------------------------------------------

def append_tweets(existing_tweets, new_tweets):
    if not new_tweets:
        return existing_tweets

    existing_ids = {t.get("id_str") for t in existing_tweets if t.get("id_str")}
    unique_new = [t for t in new_tweets if t.get("id_str") not in existing_ids]
    if not unique_new:
        return existing_tweets

    result = unique_new + existing_tweets
    print(f"âœ¨ Appended: {len(unique_new)} new tweets to head")
    return result

# ---------------------------------------------------------------------------
# Gistä½œæˆãƒ»ç§»å‹•
# ---------------------------------------------------------------------------

def create_gist_for_user(user, tweets):
    """æ–°ã—ã„Gistã‚’ä½œæˆã—ã¦ãƒ¦ãƒ¼ã‚¶ã®ãƒ„ã‚¤ãƒ¼ãƒˆã‚’æ ¼ç´ã—ã€Gist IDã‚’è¿”ã™"""
    data = {
        "user_screen_name": user,
        "user_gists": {},
        "tweets": tweets
    }
    fd, tmp_file = tempfile.mkstemp(suffix=".json", prefix=f"new_gist_{user}_")
    try:
        with os.fdopen(fd, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        result = subprocess.run(
            ["gh", "gist", "create", tmp_file, "-p",
             "--filename", "data.json",
             "-d", f"Gallery Data for @{user}"],
            capture_output=True, text=True,
        )
    finally:
        if os.path.exists(tmp_file):
            os.unlink(tmp_file)

    if result.returncode != 0:
        print(f"âŒ æ–°è¦Gistä½œæˆå¤±æ•—: {result.stderr}")
        sys.exit(1)

    gist_url = result.stdout.strip().rstrip("/")
    new_gist_id = gist_url.split("/")[-1]
    return new_gist_id

def update_or_migrate_user_gist(promote_gist_id, promote_filename, promote_data, user, merged_tweets):
    """ãƒ¦ãƒ¼ã‚¶Gistã‚’æ›´æ–°ã™ã‚‹ã€‚GIST_MAX_TWEETSã‚’è¶…ãˆã‚‹å ´åˆã¯æ–°è¦Gistã‚’ä½œæˆã—ç§»å‹•ã™ã‚‹ã€‚"""
    if not is_master_gist_format(promote_data):
        print(f"âš ï¸  è­¦å‘Š: ç§»å‹•å…ˆGist {promote_gist_id} ãŒMasterå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã—ã¤ã¤å¤‰æ›ã—ã¾ã™ã€‚")
        legacy_tweets = []
        if isinstance(promote_data, dict):
            if "users" in promote_data:
                for u_data in promote_data["users"].values():
                    legacy_tweets.extend(u_data.get("tweets", []))
            else:
                legacy_tweets.extend(promote_data.get("tweets", []))
        elif isinstance(promote_data, list):
            legacy_tweets.extend(promote_data)
        promote_data = {"user_screen_name": "", "user_gists": {}, "tweets": legacy_tweets}

    # ä»–ãƒ¦ãƒ¼ã‚¶ã®ãƒ„ã‚¤ãƒ¼ãƒˆã‚‚å«ã‚ãŸåˆè¨ˆä»¶æ•°ã‚’è¨ˆç®—
    other_tweets = [t for t in promote_data.get("tweets", []) if not _tweet_belongs_to_user(t, user)]
    current_total = len(other_tweets)

    if current_total + len(merged_tweets) > GIST_MAX_TWEETS:
        print(f"âš ï¸  è¿½åŠ ã™ã‚‹ã¨ {GIST_MAX_TWEETS} ä»¶ã‚’è¶…ãˆã‚‹ãŸã‚ã€æ–°è¦Gistã‚’ä½œæˆã—ã¾ã™...")
        new_gist_id = create_gist_for_user(user, merged_tweets)
        print(f"ğŸ†• æ–°è¦Gistä½œæˆ: {new_gist_id}  (@{user}: {len(merged_tweets)} ä»¶)")
        
        # å¤ã„Gistã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã™ã‚‹
        updated_promote = dict(promote_data)
        updated_promote["tweets"] = other_tweets
        fd, tmp_file = tempfile.mkstemp(suffix=".json", prefix="promote_del_")
        try:
            with os.fdopen(fd, 'w', encoding='utf-8') as f:
                json.dump(updated_promote, f, ensure_ascii=False, indent=2)
            subprocess.run(["gh", "gist", "edit", promote_gist_id, "-f", promote_filename, tmp_file])
            print(f"ğŸ§¹ å¤ã„Gist ({promote_gist_id}) ã‹ã‚‰ @{user} ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
        finally:
            if os.path.exists(tmp_file):
                os.unlink(tmp_file)
        
        return new_gist_id

    # æ—¢å­˜ã® promote Gist ã«è¿½åŠ 
    updated_promote = dict(promote_data)
    updated_promote["tweets"] = merged_tweets + other_tweets
    
    fd, tmp_file = tempfile.mkstemp(suffix=".json", prefix="promote_upd_")
    try:
        with os.fdopen(fd, 'w', encoding='utf-8') as f:
            json.dump(updated_promote, f, ensure_ascii=False, indent=2)
        result = subprocess.run(
            ["gh", "gist", "edit", promote_gist_id, "-f", promote_filename, tmp_file],
            capture_output=True, text=True,
        )
    finally:
        if os.path.exists(tmp_file):
            os.unlink(tmp_file)

    if result.returncode != 0:
        print(f"âŒ å¯¾è±¡ãƒ¦ãƒ¼ã‚¶Gistæ›´æ–°å¤±æ•—: {result.stderr}")
        sys.exit(1)

    after_total = current_total + len(merged_tweets)
    print(f"âœ… @{user} ã®ãƒ‡ãƒ¼ã‚¿ã‚’ Gist {promote_gist_id} ã«ä¿å­˜ã—ã¾ã—ãŸ (åˆè¨ˆ {after_total} ä»¶)")
    return promote_gist_id

# ---------------------------------------------------------------------------
# ãƒ¡ã‚¤ãƒ³
# ---------------------------------------------------------------------------

def process_multi_user_append(master_data, new_tweets, promote_gist_id_override=None):
    """
    è¤‡æ•°ãƒ¦ãƒ¼ã‚¶ãŒæ··åœ¨ã™ã‚‹ new_tweets ã‚’ãƒã‚¹ã‚¿ãƒ¼Gistãƒ‡ãƒ¼ã‚¿ã«åæ˜ ã™ã‚‹
    """
    user_groups = group_tweets_by_user(new_tweets)
    print(f"ğŸ‘¥ Users in extracted data: {len(user_groups)}")

    user_gists_map = master_data.get("user_gists", {})
    master_tweets = master_data.get("tweets", [])
    
    migrated_count = 0

    for user, tweets in user_groups.items():
        if user == "Unknown":
            master_tweets = append_tweets(master_tweets, tweets)
            continue
            
        print(f"--- Processing @{user} ({len(tweets)} new tweets) ---")

        promote_gist_id = promote_gist_id_override
        is_existing_user = user in user_gists_map
        if not promote_gist_id:
            if is_existing_user:
                promote_gist_id = user_gists_map[user]
            else:
                promote_gist_id = select_promote_gist_from_master(master_data)

        if not promote_gist_id:
            print("âš ï¸ æ—¢å­˜ã®ãƒ¦ãƒ¼ã‚¶GistãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
            promote_gist_id = create_gist_for_user(user, [])

        promote_filename, promote_data = fetch_gist_data(promote_gist_id)
        existing_tweets = get_user_tweets(promote_data, user)
        
        merged = append_tweets(existing_tweets, tweets)
        
        if len(merged) == len(existing_tweets):
            print(f"â„¹ï¸ @{user}: å…¨ã¦æ—¢å­˜ãƒã‚¹ãƒˆã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue
            
        migrated_count += (len(merged) - len(existing_tweets))
        final_user_gist_id = update_or_migrate_user_gist(promote_gist_id, promote_filename, promote_data, user, merged)

        user_gists_map[user] = final_user_gist_id
        master_tweets = [t for t in master_tweets if extract_username(t) != user]
        
        rep = dict(merged[0])
        rep["gist_id"] = final_user_gist_id
        master_tweets.insert(0, rep)

    print(f"ğŸ“Š Total migrated to user Gists: {migrated_count} tweets")
    master_data["user_gists"] = user_gists_map
    master_data["tweets"] = master_tweets
    return master_data


def main():
    args = parse_args()

    if not args.user and not args.hashtag and not args.foryou:
        print("âŒ Error: --user, --hashtag ã¾ãŸã¯ --foryou ã®ã„ãšã‚Œã‹ãŒå¿…è¦ã§ã™ã€‚")
        sys.exit(1)

    target_label = "ForYou" if args.foryou else (f"#{args.hashtag}" if args.hashtag else f"@{args.user}")
    print(f"ğŸ¯ Target: {target_label}")

    # 1. æ—¢å­˜Gistãƒ‡ãƒ¼ã‚¿å–å¾—
    gist_filename, full_data = fetch_gist_data(args.gist_id)

    # 2. ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¼·åˆ¶
    if not is_master_gist_format(full_data):
        print(f"âŒ Error: Gist {args.gist_id} is not in Master format (missing 'user_gists').")
        print("Support for other formats has been removed. Please use a master Gist.")
        sys.exit(1)
    
    # 3. æ—¢å­˜IDæŠ½å‡º
    skip_ids_file = ""
    if args.user and not args.foryou:
        user_gists = full_data.get("user_gists", {})
        if args.user in user_gists:
            _, p_data = fetch_gist_data(user_gists[args.user])
            existing_tweets = get_user_tweets(p_data, args.user)
        else:
            existing_tweets = []
        skip_ids_file = write_skip_ids_file(get_existing_ids_ordered(existing_tweets))
    else:
        # è¤‡æ•°ãƒ¦ãƒ¼ã‚¶ï¼ˆforyou/hashtagï¼‰ã®å ´åˆã¯å…¨ä»£è¡¨ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ç”¨ã«ã™ã‚‹
        skip_ids_file = write_skip_ids_file(get_existing_ids_ordered(full_data.get("tweets", [])))

    # 4. æ–°è¦ãƒã‚¹ãƒˆå–å¾—
    try:
        run_extraction(args, skip_ids_file)
    finally:
        if os.path.exists(skip_ids_file):
            os.unlink(skip_ids_file)

    new_tweets = parse_tweets_js()
    print(f"ğŸ“¥ New tweets extracted: {len(new_tweets)}")
    if not new_tweets:
        print("âœ… å–å¾—ã§ããŸæ–°è¦ãƒ„ã‚¤ãƒ¼ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        sys.exit(0)

    # 5. ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ï¼ˆå¸¸ã«ãƒ—ãƒ­ã‚»ã‚¹çµŒç”±ï¼‰
    final_output = process_multi_user_append(full_data, new_tweets, args.promote_gist_id)

    # 6. ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜
    output_file = "assets/data/data.json"
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_output, f, ensure_ascii=False, indent=2)

    # 7. Gistæ›´æ–°
    print(f"â˜ï¸ Updating Gist ({args.gist_id})...")
    result = subprocess.run(
        ["gh", "gist", "edit", args.gist_id, "-f", gist_filename, output_file],
        capture_output=True, text=True,
    )
    if result.returncode == 0:
        print(f"âœ… Gist updated successfully! Target: {target_label}")
    else:
        print(f"âŒ Gist update failed: {result.stderr}")
        sys.exit(1)

if __name__ == "__main__":
    main()
