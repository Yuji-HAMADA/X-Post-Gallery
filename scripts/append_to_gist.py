"""
æŒ‡å®šãƒ¦ãƒ¼ã‚¶(ã¾ãŸã¯ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°)ã®ãƒã‚¹ãƒˆã‚’å–å¾—ã—ã€æ—¢å­˜Gistã«ã‚¢ãƒšãƒ³ãƒ‰ã™ã‚‹ã€‚
  - å…ˆé ­1ä»¶ã¯Gistã®å…ˆé ­ã«æŒ¿å…¥ã€æ®‹ã‚Šã¯æœ«å°¾ã«è¿½åŠ 
  - æ—¢å­˜IDãŒè¦‹ã¤ã‹ã£ãŸã‚‰å–å¾—ã‚’åœæ­¢ï¼ˆ--stop-on-existingï¼‰
"""
import json
import os
import re
import sys
import argparse
import subprocess
import tempfile

DATA_DIR = "data"
TWEETS_JS = os.path.join(DATA_DIR, "tweets.js")

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-g", "--gist-id", required=True, help="Appendå¯¾è±¡ã®Gist ID")
    parser.add_argument("-u", "--user", default=None, help="Target user ID (--user ã¾ãŸã¯ --hashtag ã®ã©ã¡ã‚‰ã‹å¿…é ˆ)")
    parser.add_argument("--hashtag", type=str, default=None, help="Target hashtag (#ãªã—)")
    parser.add_argument("-m", "--mode", default="post_only", choices=["all", "post_only"])
    parser.add_argument("-n", "--num", type=int, default=100, help="æœ€å¤§å–å¾—ä»¶æ•°")
    parser.add_argument("-s", "--stop-on-existing", action="store_true", help="æ—¢å­˜IDã«å½“ãŸã£ãŸã‚‰åœæ­¢ï¼ˆã‚¹ãƒˆãƒƒãƒ—ã‚ªãƒ³ãƒ¢ãƒ¼ãƒ‰ï¼‰")
    parser.add_argument("--force-empty", action="store_true", help="GistãŒ0ä»¶ã§ã‚‚å¼·åˆ¶ç¶šè¡Œï¼ˆé€šå¸¸ã¯å®‰å…¨ã®ãŸã‚ä¸­æ–­ï¼‰")
    return parser.parse_args()

def fetch_gist_data(gist_id):
    """Gistã‹ã‚‰JSONå–å¾—ã€‚ãƒ•ã‚¡ã‚¤ãƒ«åã¨æ—¢å­˜tweetsãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚
    å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã¯ gh gist view ã§åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã‚‹ãŸã‚ã€APIçµŒç”±ã§raw_urlã‚’å–å¾—ã—curlã§è½ã¨ã™ã€‚
    """
    try:
        result = subprocess.run(
            ["gh", "api", f"gists/{gist_id}"],
            capture_output=True, text=True, check=True,
        )
        gist_meta = json.loads(result.stdout)
    except (subprocess.CalledProcessError, json.JSONDecodeError) as e:
        print(f"âŒ Failed to fetch Gist metadata: {e}")
        sys.exit(1)

    candidate_files = ["data.json", "gallary_data.json"]
    files = gist_meta.get("files", {})
    for filename in candidate_files:
        if filename not in files:
            continue
        raw_url = files[filename].get("raw_url")
        if not raw_url:
            continue
        try:
            dl = subprocess.run(
                ["curl", "-sL", raw_url],
                capture_output=True, text=True, check=True,
            )
            data = json.loads(dl.stdout)
            tweets = data.get("tweets", []) if isinstance(data, dict) else data
            user_screen_name = data.get("user_screen_name", "Unknown") if isinstance(data, dict) else "Unknown"
            print(f"â˜ï¸ Gist: {len(tweets)} items ('{filename}')")
            return filename, user_screen_name, tweets
        except (subprocess.CalledProcessError, json.JSONDecodeError) as e:
            print(f"âš ï¸ Failed to parse '{filename}': {e}")
            continue
    print("âŒ No valid data found in Gist.")
    sys.exit(1)

def get_existing_ids_ordered(tweets):
    """æ—¢å­˜tweetsã‹ã‚‰é †åºä»˜ãIDãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰ï¼ˆé€£ç¶šä¸€è‡´åˆ¤å®šç”¨ï¼‰"""
    ids = []
    seen = set()
    for item in tweets:
        tid = item.get("id_str") or item.get("tweet", {}).get("id_str")
        if tid and tid not in seen:
            ids.append(tid)
            seen.add(tid)
    return ids

def write_skip_ids_file(ordered_ids):
    """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ—¢å­˜IDã‚’é †åºä»˜ãã§æ›¸ãå‡ºã™ï¼ˆé€£ç¶šä¸€è‡´åˆ¤å®šç”¨ï¼‰"""
    fd, path = tempfile.mkstemp(suffix=".txt", prefix="skip_ids_")
    with os.fdopen(fd, 'w') as f:
        for tid in ordered_ids:
            f.write(tid + "\n")
    return path

def run_extraction(user, hashtag, mode, num, skip_ids_file, stop_on_existing=True):
    """extract_media.py ã‚’å‘¼ã³å‡ºã—ã¦ãƒã‚¹ãƒˆã‚’å–å¾—"""
    cmd = [
        sys.executable, "scripts/extract_media.py",
        "--mode", mode,
        "-n", str(num),
        "--skip-ids-file", skip_ids_file,
    ]
    if user:
        cmd.extend(["-u", user])
    elif hashtag:
        cmd.extend(["--hashtag", hashtag])
    if stop_on_existing:
        cmd.append("--stop-on-existing")
    print(f"ğŸš€ Running: {' '.join(cmd)}")
    result = subprocess.run(cmd)
    if result.returncode != 0:
        print("âŒ Extraction failed.")
        sys.exit(1)

def parse_tweets_js():
    """extract_media.py ãŒå‡ºåŠ›ã—ãŸ tweets.js ã‚’èª­ã‚€"""
    if not os.path.exists(TWEETS_JS):
        print("âŒ tweets.js not found.")
        sys.exit(1)
    with open(TWEETS_JS, 'r', encoding='utf-8') as f:
        content = f.read()
    json_str = re.sub(r'^window\.YTD\.tweets\.part0\s*=\s*', '', content)
    raw_tweets = json.loads(json_str)

    # update_data.py ã¨åŒã˜å¤‰æ›ãƒ­ã‚¸ãƒƒã‚¯
    converted = []
    for item in raw_tweets:
        tweet = item.get('tweet', {})
        full_text = tweet.get('full_text', '')
        media_list = tweet.get('extended_entities', {}).get('media', [])
        if not media_list:
            media_list = tweet.get('entities', {}).get('media', [])
        if not media_list:
            continue
        media_urls = [m.get('media_url_https', '') for m in media_list if m.get('media_url_https')]
        if not media_urls:
            continue

        post_url = ''
        for m in media_list:
            eu = m.get('expanded_url', '')
            if '/status/' in eu:
                post_url = re.sub(r'/photo/\d+$', '', eu)
                break

        entry = {
            'full_text': full_text,
            'created_at': tweet.get('created_at', ''),
            'media_urls': media_urls,
            'id_str': tweet.get('id_str', ''),
        }
        if post_url:
            entry['post_url'] = post_url
        converted.append(entry)

    return converted

def append_tweets(existing_tweets, new_tweets):
    """æ–°è¦tweetsã‚’æ—¢å­˜ã«æŒ¿å…¥: å…ˆé ­1ä»¶â†’å…ˆé ­ã€æ®‹ã‚Šâ†’æœ«å°¾"""
    if not new_tweets:
        print("â„¹ï¸ No new tweets to append.")
        return existing_tweets

    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨
    existing_ids = {t.get("id_str") for t in existing_tweets if t.get("id_str")}

    unique_new = [t for t in new_tweets if t.get("id_str") not in existing_ids]
    if not unique_new:
        print("â„¹ï¸ All tweets already exist. Nothing to append.")
        return existing_tweets

    first = unique_new[0]
    rest = unique_new[1:]

    # å…ˆé ­1ä»¶ã‚’Gistã®å…ˆé ­ã«ã€æ®‹ã‚Šã‚’æœ«å°¾ã«
    result = [first] + existing_tweets + rest
    print(f"âœ¨ Appended: 1 to head + {len(rest)} to tail = {len(unique_new)} new tweets")
    return result

def main():
    args = parse_args()

    if not args.user and not args.hashtag:
        print("âŒ Error: --user ã¾ãŸã¯ --hashtag ã®ã©ã¡ã‚‰ã‹ãŒå¿…è¦ã§ã™ã€‚")
        sys.exit(1)

    target_label = f"#{args.hashtag}" if args.hashtag else f"@{args.user}"
    print(f"ğŸ¯ Target: {target_label}")

    # 1. æ—¢å­˜Gistãƒ‡ãƒ¼ã‚¿å–å¾—
    gist_filename, user_screen_name, existing_tweets = fetch_gist_data(args.gist_id)
    existing_ids_ordered = get_existing_ids_ordered(existing_tweets)
    print(f"ğŸ“‹ Existing IDs: {len(existing_ids_ordered)}")

    # å®‰å…¨ãƒã‚§ãƒƒã‚¯: Appendãªã®ã«0ä»¶ã¯ç•°å¸¸ã€‚ä¸Šæ›¸ãäº‹æ•…ã‚’é˜²ããŸã‚ä¸­æ–­ã™ã‚‹
    if len(existing_tweets) == 0 and not args.force_empty:
        print("âš ï¸  è­¦å‘Š: Gistã®Tweetæ•°ãŒ0ä»¶ã§ã™ã€‚")
        print("   Appendãƒ¢ãƒ¼ãƒ‰ãªã®ã«æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒç©ºãªã®ã¯ç•°å¸¸ãªçŠ¶æ…‹ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        print("   æ„å›³çš„ã«ç©ºã®Gistã¸Appendã—ãŸã„å ´åˆã¯ --force-empty ã‚’ä»˜ã‘ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    # 2. æ—¢å­˜IDãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆï¼ˆé †åºä»˜ãï¼šé€£ç¶šä¸€è‡´åˆ¤å®šç”¨ï¼‰
    skip_ids_file = write_skip_ids_file(existing_ids_ordered)

    try:
        # 3. æ–°è¦ãƒã‚¹ãƒˆå–å¾—
        run_extraction(args.user, args.hashtag, args.mode, args.num, skip_ids_file, args.stop_on_existing)
    finally:
        os.unlink(skip_ids_file)

    # 4. å–å¾—çµæœã‚’ãƒ‘ãƒ¼ã‚¹
    new_tweets = parse_tweets_js()
    print(f"ğŸ“¥ New tweets extracted: {len(new_tweets)}")

    # 5. ã‚¢ãƒšãƒ³ãƒ‰
    merged = append_tweets(existing_tweets, new_tweets)

    # 6. ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜
    output_file = "assets/data/data.json"
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    final_output = {
        "user_screen_name": user_screen_name,
        "tweets": merged,
    }
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_output, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ Saved: {len(merged)} tweets to {output_file}")

    # 7. Gistæ›´æ–°
    print(f"â˜ï¸ Updating Gist ({args.gist_id})...")
    # gh gist edit ã¯ -f <filename> <local_file> ã®å½¢å¼
    result = subprocess.run(
        ["gh", "gist", "edit", args.gist_id, "-f", gist_filename, output_file],
        capture_output=True, text=True,
    )
    if result.returncode == 0:
        print(f"âœ… Gist updated successfully! Total: {len(merged)} tweets")
    else:
        print(f"âŒ Gist update failed: {result.stderr}")
        sys.exit(1)

if __name__ == "__main__":
    main()
